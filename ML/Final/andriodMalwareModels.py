#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Aug 31 14:59:06 2023

@author: domin
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.svm import SVR
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier


# Returns array containing the
#   data as an array, target column, Target Name, and Features
def loadCSV(filename):
    arr = np.loadtxt(filename, delimiter=",", dtype=str)
    return arr

def pruneData(data):
    return data[0:5000]

def getHeader(data):
    return data[0], data[1:].astype(int)

def splitData(data):
    target = data[:,0]
    data = np.delete(data, obj=0, axis=1)
    return data, target

def DecisionTree_Model(df, targ):
    X_train, X_test, y_train, y_test = train_test_split(
        df, targ, stratify=targ, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    training_accuracy = []
    test_accuracy = []
    #try n_neighbors from 1 to 50
    depth_settings = range(1, 7)

    for max_depth in depth_settings:
        #build the model
        model = DecisionTreeClassifier(random_state=42, max_depth=max_depth)
        model.fit(X_train, y_train)
        #record training set accuracy
        training_accuracy.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))

    print("Decision Tree:")
    metrics(depth_settings, training_accuracy, test_accuracy, "max depth")
    evaluate(X_train, X_test, y_train, y_test, model)

def RandomForest_Model(df, targ):
    # split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        df, targ, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    X_test = [list(map(int, x)) for x in X_test]
    training_accuracy = []
    test_accuracy = []
    estimators_settings = range(1, 21)
    for n_estimators in estimators_settings:
        # create the model
        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)
        # train the model
        model.fit(X_train, y_train)
        # transpose y_test
        y_test = np.array(y_test.astype(int)).T
        #record training set accuracy
        training_accuracy.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))

    print('Random Forest:')
    print('R2 score', model.score(X_test, y_test))

    metrics(estimators_settings, training_accuracy, test_accuracy, "n_estimators")
    evaluate(X_train, X_test, y_train, y_test, model)

def GradientBoosted_Model(df, targ):
    # split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        df, targ, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    X_test = [list(map(int, x)) for x in X_test]
    training_accuracy1 = []
    training_accuracy2 = []
    test_accuracy1 = []
    test_accuracy2 = []
    estimators_settings = range(1, 7)
    learningrate_settings = np.arange(.8, .99, .01)
    for n_estimators in estimators_settings:
        # create the model
        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, random_state=42)
        # train the model
        model.fit(X_train, y_train)
        # transpose y_test
        y_test = np.array(y_test.astype(int)).T
        #record training set accuracy
        training_accuracy1.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy1.append(model.score(X_test, y_test))
    for n_learningrates in learningrate_settings:
        # create the model
        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=n_learningrates, random_state=42)
        # train the model
        model.fit(X_train, y_train)
        # transpose y_test
        y_test = np.array(y_test.astype(int)).T
        #record training set accuracy
        training_accuracy2.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy2.append(model.score(X_test, y_test))

    print('GradientBoosted:')
    print('MeanSquared error = ', model.score(X_test, y_test))

    metrics(estimators_settings, training_accuracy1, test_accuracy1, "n_estimators")
    metrics(learningrate_settings, training_accuracy2, test_accuracy2, "learning rate")
    evaluate(X_train, X_test, y_train, y_test, model)

def KNN_Model(arr, target):
    X_train, X_test, y_train, y_test = train_test_split(
        arr, target, stratify=target, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    training_accuracy = []
    test_accuracy = []
    #try n_neighbors from 1 to 50
    neighbors_settings = range(1, 7)

    for n_neighbors in neighbors_settings:
        #build the model
        model = KNeighborsClassifier(n_neighbors=n_neighbors)
        model.fit(X_train, y_train)
        #record training set accuracy
        training_accuracy.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))

    print("KNN:")
    metrics(neighbors_settings, training_accuracy, test_accuracy, "n_neighbors")
    evaluate(X_train, X_test, y_train, y_test, model)

def LinearDiscriminantAnalysis_Model(arr, target):
    X_train, X_test, y_train, y_test = train_test_split(
        arr, target, stratify=target, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    training_accuracy = []
    test_accuracy = []
    #try n_neighbors from 1 to 50
    solver_settings = ['svd', 'lsqr']

    for solver in solver_settings:
        #build the model
        model = LinearDiscriminantAnalysis(solver=solver)
        model.fit(X_train, y_train)
        #record training set accuracy
        training_accuracy.append(model.score(X_train, y_train))
        #record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))

    print("Linear Discriminant Analysis:")
    metrics(solver_settings, training_accuracy, test_accuracy, "solver")
    evaluate(X_train, X_test, y_train, y_test, model)

def NN_Model(df, targ):
    # split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        df, targ, stratify=targ, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    training_accuracy = []
    test_accuracy = []
    estimators_settings = range(1, 10)
    for n_estimators in estimators_settings:
        # create the model
        model = MLPClassifier(hidden_layer_sizes= n_estimators, activation='relu', max_iter=10000)
        # train the model
        model.fit(X_train, y_train)
        # record training set accuracy
        training_accuracy.append(model.score(X_train, y_train))
        # record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))

    print("Neural Network: ")

    metrics(estimators_settings, training_accuracy, test_accuracy, "n_estimators")
    evaluateNN(X_train, X_test, y_train, y_test, model)


def SVR_Model(df, targ):
  # split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        df, targ, stratify=targ, shuffle=True, random_state=42, train_size=.8, test_size=.2)
    training_accuracy = []
    test_accuracy = []
    parameters = []
    best_score = 0
    c_settings = [0.1, 1, 10, 20, 50, 100]
    for gamma in [0.1, 1, 10, 20, 50, 100]:
      for C in [0.1, 1, 10, 20, 50, 100]:
        model = SVR(gamma=gamma, C=C)
        scores = cross_val_score(model, X_train, y_train, cv=5)
        score = np.mean(scores)
        parameters.append([C, gamma, score])
        if score > best_score:
            best_score = score
            best_parameters = {'C': C, 'gamma': gamma}
        # record training set accuracy
        model.fit(X_train, y_train)
        training_accuracy.append(model.score(X_train, y_train))
        # record generalization accuracy
        test_accuracy.append(model.score(X_test, y_test))
    model = SVR(**best_parameters)
    model.fit(X_train, y_train)

    

    print("SVR Model: ")
    
    metrics(c_settings, training_accuracy[0:len(c_settings)], test_accuracy[0:len(c_settings)], "C")
    evaluateNN(X_train, X_test, y_train, y_test, model)


def metrics(settings, training_accuracy, test_accuracy, label):
    plt.plot(settings, training_accuracy, label="training accuracy")
    plt.plot(settings, test_accuracy, label="test accuracy")
    plt.ylabel("accuracy")
    plt.xlabel(label)
    plt.legend()
    plt.show()


def evaluate(X_train, X_test, y_train, y_test, model):
    print("Precision:",     precision_score(y_test, model.predict(X_test)))
    print("Recall:",        recall_score(y_test,    model.predict(X_test)))
    print("Sensitivity:",   recall_score(y_test,    model.predict(X_test)))
    print("Accuracy:",      accuracy_score(y_test,  model.predict(X_test)))
    print("F Statistic:",   f1_score(y_test,        model.predict(X_test)))
    print("\n")

def evaluateNN(X_train, X_test, y_train, y_test, model):
    print("Mean Absolute Error:",     mean_absolute_error(y_test, model.predict(X_test)))
    print("Mean Squared Error:",      mean_squared_error(y_test,    model.predict(X_test)))
    print("Root Mean Squared Error:", np.sqrt(mean_squared_error(y_test,    model.predict(X_test))))
    print("R-squared:",               r2_score(y_test,  model.predict(X_test)))
    print("\n")


def main():
    data = loadCSV("data.csv")
    #data = pruneData(data)
    header, data = getHeader(data)
    target = splitData(data)[1]
    data = np.delete(data, obj=0, axis=1)
    KNN_Model(data, target)
    DecisionTree_Model(data, target)
    #RandomForest_Model(data, target)
    #GradientBoosted_Model(data, target)
    #LinearDiscriminantAnalysis_Model(data, target)
    #NN_Model(data, target)
    #SVR_Model(data, target)
    
main()